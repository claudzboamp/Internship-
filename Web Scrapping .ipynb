{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54cd7c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n"
     ]
    }
   ],
   "source": [
    "#1, \n",
    "\n",
    "#Install Libraries \n",
    "\n",
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817b1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545d79f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 text level\n",
      "0  Wikipedia\\n\\nThe Free Encyclopedia    h1\n",
      "1            1 000 000+\\n\\n\\narticles    h2\n",
      "2              100 000+\\n\\n\\narticles    h2\n",
      "3               10 000+\\n\\n\\narticles    h2\n",
      "4                1 000+\\n\\n\\narticles    h2\n",
      "5                  100+\\n\\n\\narticles    h2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make a GET request to wikipedia.org and store the response\n",
    "page = requests.get(\"https://www.wikipedia.org/\")\n",
    "\n",
    "# Parse the response content as HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find all the header tags (h1, h2, h3, h4, h5, h6) in the HTML and store them in a list\n",
    "headers = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "\n",
    "# Create an empty list to store the header text and level\n",
    "header_list = []\n",
    "\n",
    "# Loop through the headers list and append a tuple of (header text, header level) to the header_list\n",
    "for header in headers:\n",
    "    header_text = header.get_text().strip() # Get the text of the header and remove any leading or trailing whitespace\n",
    "    header_level = header.name # Get the name of the header tag (e.g. h1, h2, etc.)\n",
    "    header_list.append((header_text, header_level))\n",
    "\n",
    "# Create a pandas data frame from the header_list with columns \"text\" and \"level\"\n",
    "df = pd.DataFrame(header_list, columns=[\"text\", \"level\"])\n",
    "\n",
    "# Display the data frame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ff675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3, Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9ada641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pos              team matches points rating\n",
      "0   1        India\\nIND      29  3,434    118\n",
      "1   2    Australia\\nAUS      30  3,534    118\n",
      "2   3      England\\nENG      43  4,941    115\n",
      "3   4  South Africa\\nSA      21  2,182    104\n",
      "4   5   New Zealand\\nNZ      23  2,291    100\n",
      "5   6     Pakistan\\nPAK      25  2,304     92\n",
      "6   7     Sri Lanka\\nSL      27  2,123     79\n",
      "7   8   West Indies\\nWI      28  2,154     77\n",
      "8   9   Bangladesh\\nBAN      19    873     46\n",
      "9  10     Zimbabwe\\nZIM       7    223     32\n"
     ]
    }
   ],
   "source": [
    "# Import requests and BeautifulSoup libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/test\"\n",
    "\n",
    "# Make a GET request to the URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the response status code is 200 (OK)\n",
    "if response.status_code == 200:\n",
    "    # Parse the response content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table element with class \"table\"\n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "    # Find all the table row elements within the table\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    # Create an empty list to store the rankings\n",
    "    rankings = []\n",
    "\n",
    "    # Loop through each row element, skipping the first one (header row)\n",
    "    for row in rows[1:]:\n",
    "        # Find all the table data elements within the row\n",
    "        data = row.find_all(\"td\")\n",
    "\n",
    "        # Get the text content of each data element\n",
    "        pos = data[0].get_text().strip()\n",
    "        team = data[1].get_text().strip()\n",
    "        matches = data[2].get_text().strip()\n",
    "        points = data[3].get_text().strip()\n",
    "        rating = data[4].get_text().strip()\n",
    "\n",
    "        # Append a dictionary with the data to the rankings list\n",
    "        rankings.append({\"pos\": pos, \"team\": team, \"matches\": matches, \"points\": points, \"rating\": rating})\n",
    "\n",
    "    # Create a pandas data frame from the rankings list\n",
    "    df = pd.DataFrame(rankings)\n",
    "\n",
    "    # Print the data frame\n",
    "    print(df)\n",
    "else:\n",
    "    # Print an error message if the response status code is not 200\n",
    "    print(f\"Error: Could not scrape the URL. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81e8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category a)\n",
      "  pos              team matches points rating\n",
      "0   1    Australia\\nAUS      26  4,290    165\n",
      "1   2      England\\nENG      31  3,875    125\n",
      "2   3  South Africa\\nSA      26  3,098    119\n",
      "3   4        India\\nIND      30  3,039    101\n",
      "4   5   New Zealand\\nNZ      28  2,688     96\n",
      "5   6   West Indies\\nWI      29  2,743     95\n",
      "6   7   Bangladesh\\nBAN      17  1,284     76\n",
      "7   8     Sri Lanka\\nSL      12    820     68\n",
      "8   9     Thailand\\nTHA      13    883     68\n",
      "9  10     Pakistan\\nPAK      27  1,678     62\n",
      "Category b)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m category \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m category \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     71\u001b[0m     pos \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 72\u001b[0m     player \u001b[38;5;241m=\u001b[39m \u001b[43mcols\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     73\u001b[0m     team \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     74\u001b[0m     rating \u001b[38;5;241m=\u001b[39m cols[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Import requests and BeautifulSoup libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base URL to scrape\n",
    "base_url = \"https://www.icc-cricket.com/rankings/womens\"\n",
    "\n",
    "# Define the sub-URLs for each category\n",
    "sub_urls = {\n",
    "    \"a\": \"/team-rankings/odi\",\n",
    "    \"b\": \"/player-rankings/odi/batting\",\n",
    "    \"c\": \"/player-rankings/odi/all-rounder\"\n",
    "}\n",
    "\n",
    "# Define the table class for each category\n",
    "table_class = {\n",
    "    \"a\": \"table\",\n",
    "    \"b\": \"table rankings-table\",\n",
    "    \"c\": \"table rankings-table\"\n",
    "}\n",
    "\n",
    "# Define the number of rows to scrape for each category\n",
    "num_rows = {\n",
    "    \"a\": 10,\n",
    "    \"b\": 10,\n",
    "    \"c\": 10\n",
    "}\n",
    "\n",
    "# Loop through each category\n",
    "for category in sub_urls:\n",
    "    # Print the category name\n",
    "    print(f\"Category {category})\")\n",
    "\n",
    "    # Construct the full URL by joining the base and sub URLs\n",
    "    url = base_url + sub_urls[category]\n",
    "\n",
    "    # Make a GET request to the URL and store the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the response status code is 200 (OK)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find the table element with the specified class\n",
    "        table = soup.find(\"table\", class_=table_class[category])\n",
    "\n",
    "        # Find all the table row elements within the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row element, up to the specified number of rows\n",
    "        for row in rows[1:num_rows[category]+1]:\n",
    "            # Find all the table data elements within the row\n",
    "            cols = row.find_all(\"td\")\n",
    "\n",
    "            # Get the text content of each data element, depending on the category\n",
    "            if category == \"a\":\n",
    "                pos = cols[0].get_text().strip()\n",
    "                team = cols[1].get_text().strip()\n",
    "                matches = cols[2].get_text().strip()\n",
    "                points = cols[3].get_text().strip()\n",
    "                rating = cols[4].get_text().strip()\n",
    "\n",
    "                # Append a dictionary with the data to the data list\n",
    "                data.append({\"pos\": pos, \"team\": team, \"matches\": matches, \"points\": points, \"rating\": rating})\n",
    "            elif category == \"b\" or category == \"c\":\n",
    "                pos = cols[0].get_text().strip()\n",
    "                player = cols[1].find(\"div\", class_=\"name\").get_text().strip()\n",
    "                team = cols[1].find(\"div\", class_=\"country\").get_text().strip()\n",
    "                rating = cols[2].get_text().strip()\n",
    "\n",
    "                # Append a dictionary with the data to the data list\n",
    "                data.append({\"pos\": pos, \"player\": player, \"team\": team, \"rating\": rating})\n",
    "\n",
    "        # Create a pandas data frame from the data list\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Print the data frame\n",
    "        print(df)\n",
    "    else:\n",
    "        # Print an error message if the response status code is not 200\n",
    "        print(f\"Error: Could not scrape the URL. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d39000ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Loop through each div element and extract the headline text\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m news_divs:\n\u001b[1;32m---> 25\u001b[0m     headline \u001b[38;5;241m=\u001b[39m \u001b[43mdiv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCard-title\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     26\u001b[0m     headlines\u001b[38;5;241m.\u001b[39mappend(headline)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Create a data frame from the list of headlines\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dfad968",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Looping through each div element and extracting the data\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m news_divs:\n\u001b[0;32m     25\u001b[0m   \u001b[38;5;66;03m# Getting the headline text\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m   headline \u001b[38;5;241m=\u001b[39m \u001b[43mdiv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCard-title\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     27\u001b[0m   headlines\u001b[38;5;241m.\u001b[39mappend(headline)\n\u001b[0;32m     29\u001b[0m   \u001b[38;5;66;03m# Getting the time text\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Setting the URL\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Sending a GET request and getting the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parsing the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Finding all the div elements that contain the news details\n",
    "news_divs = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
    "\n",
    "# Creating empty lists to store the scraped data\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "# Looping through each div element and extracting the data\n",
    "for div in news_divs:\n",
    "  # Getting the headline text\n",
    "  headline = div.find(\"h3\", class_=\"Card-title\").text.strip()\n",
    "  headlines.append(headline)\n",
    "\n",
    "  # Getting the time text\n",
    "  time = div.find(\"time\", class_=\"Card-time\").text.strip()\n",
    "  times.append(time)\n",
    "\n",
    "  # Getting the news link\n",
    "  link = div.find(\"a\", class_=\"Card-leadLink\")[\"href\"]\n",
    "  links.append(link)\n",
    "\n",
    "# Creating a data frame from the lists\n",
    "df = pd.DataFrame({\"Headline\": headlines, \"Time\": times, \"News Link\": links})\n",
    "\n",
    "# Printing the data frame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a043b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Creating an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Setting the base url and the headers\n",
    "base_url = \"https://www.dineout.co.in/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Looping through the pages\n",
    "for page in range(1, 11):\n",
    "    # Constructing the url for each page\n",
    "    url = base_url + \"delhi-restaurants?page=\" + str(page)\n",
    "    # Sending a get request and getting the response\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Parsing the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Finding all the restaurant cards\n",
    "    cards = soup.find_all(\"div\", class_=\"restnt-card\")\n",
    "    # Looping through each card\n",
    "    for card in cards:\n",
    "        # Getting the restaurant name\n",
    "        name = card.find(\"div\", class_=\"restnt-info cursor\").text.strip()\n",
    "        # Getting the cuisine\n",
    "        cuisine = card.find(\"span\", class_=\"double-line-ellipsis\").text.strip()\n",
    "        # Getting the location\n",
    "        location = card.find(\"div\", class_=\"restnt-loc ellipsis\").text.strip()\n",
    "        # Getting the ratings\n",
    "        ratings = card.find(\"span\", class_=\"rating\").text.strip()\n",
    "        # Getting the image url\n",
    "        image_url = card.find(\"img\", class_=\"no-img\")[\"data-original\"]\n",
    "        # Appending the data to the list as a dictionary\n",
    "        data.append({\"Restaurant name\": name, \"Cuisine\": cuisine, \"Location\": location, \"Ratings\": ratings, \"Image URL\": image_url})\n",
    "\n",
    "# Creating a data frame from the list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Printing the first 10 rows of the data frame\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "199ece35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL and send a GET request\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the article elements\n",
    "articles = soup.find_all(\"div\", class_=\"pod-listing-header\")\n",
    "\n",
    "# Create empty lists to store the scraped data\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "# Loop through each article element and extract the data\n",
    "for article in articles:\n",
    "  # Get the title text and append it to the titles list\n",
    "  title = article.find(\"h4\", class_=\"pod-listing-title\").text.strip()\n",
    "  titles.append(title)\n",
    "\n",
    "  # Get the authors text and append it to the authors list\n",
    "  author = article.find(\"span\", class_=\"pod-listing-authors\").text.strip()\n",
    "  authors.append(author)\n",
    "\n",
    "  # Get the date text and append it to the dates list\n",
    "  date = article.find(\"span\", class_=\"pod-listing-date\").text.strip()\n",
    "  dates.append(date)\n",
    "\n",
    "  # Get the URL and append it to the urls list\n",
    "  url = article.find(\"a\", class_=\"anchor-text\")[\"href\"]\n",
    "  urls.append(url)\n",
    "\n",
    "# Create a data frame from the lists\n",
    "df = pd.DataFrame({\"Paper Title\": titles, \"Authors\": authors, \"Published Date\": dates, \"Paper URL\": urls})\n",
    "\n",
    "# Print the data frame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e3993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
